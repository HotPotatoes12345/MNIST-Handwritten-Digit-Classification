{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Handwritten Digit Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook uses an increasing number of different approachs to classify handwritten digits (zero through nine) in the MNIST dataset. A detailed description of the dataset and a catolog of previous analysis on the datset can be found here: http://yann.lecun.com/exdb/mnist/. The first approach applies logistic regression as a one-vs-all multi-class classification. This approach is fully developed and acheived around an 90% accuracy, which is expected for the simplicity of the logistic regression approach. The second approach, which is still under development, utilized a 3-layer neural network with a softmax function as the final activation layer. Currenlty, the outline of the code is written explicitly, but I plan to use Tensorflow to rewrite the code of the neural network. \n",
    "\n",
    "Acknowledgement: As with many people in the machine learning community, I am indebted to Andrew Ng for his courses on Coursera with Stanford and Deeplearning.ai for teaching me how to develop machine learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideas for improvement\n",
    " - tune alpha (and lambda)\n",
    " - add regularization\n",
    " - Use other algorithms like\n",
    "     - apply neural networks w/ softmax\n",
    "     - apply SVM's\n",
    "     - apply kNN\n",
    "     - apply decision trees\n",
    " - implement Adam optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from mlxtend.data import loadlocal_mnist\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset was downloaded from http://yann.lecun.com/exdb/mnist/ and then unzipped. The data was then processed using the mlxtend.data library outlined here:  https://rasbt.github.io/mlxtend/user_guide/data/loadlocal_mnist/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_org, y_train_org = loadlocal_mnist(\n",
    "        images_path='train-images-idx3-ubyte', \n",
    "        labels_path='train-labels-idx1-ubyte')\n",
    "\n",
    "X_test_org, y_test_org = loadlocal_mnist(\n",
    "        images_path='t10k-images-idx3-ubyte', \n",
    "        labels_path='t10k-labels-idx1-ubyte')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The images of each digit are made up for 28 x 28 pixels (784 total), which have been reformatted into a single vector of length 784. There are 60,000 images in the training set and 10,000 in the test set. The y_train and y_test are oddly shaped, which is a concern, though it doesn't effect the analyis in the end. I tried to remedy this using the np.squeeze function, but it didn't work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train dimensions: 60000 x 784\n",
      "y_train dimensions:(60000,)\n",
      "X_test dimensionss: 10000 x 784\n",
      "y_test dimensions:(10000,)\n",
      "Example of X_train: [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "Example of y_train: [5 0 4 1 9 2 1 3 1 4 3 5 3 6 1]\n"
     ]
    }
   ],
   "source": [
    "print('X_train dimensions: %s x %s' % (X_train_org.shape[0], X_train_org.shape[1]))\n",
    "print('y_train dimensions:'+ str(y_train_org.shape )) \n",
    "print('X_test dimensionss: %s x %s' % (X_test_org.shape[0], X_test_org.shape[1]))\n",
    "print('y_test dimensions:'+ str(y_test_org.shape )) \n",
    "print('Example of X_train:', X_train_org[0:15,0:15])\n",
    "print('Example of y_train:', y_train_org[0:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are images of what a given example of the handwritten digits look like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x116aea550>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "example_number = 2\n",
    "img = X_train_org[example_number,:].reshape(28,28)\n",
    "plt.imshow(img, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is currently formatted with the rows being the nummber of examples, (m=60,000), and the columns being the features, (n=784). The analysis I will perform has the data transposed, with size n x m, which will be done below. Additionally, I have scaled the data to be within zero and 1 by dividing by 255, as the pixel values are 8-bit integers. \n",
    "I remedied the missing column values in y_train and y_test with by putting them inside a np.array and transposed the y-values as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 60000)\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train_org.T/255\n",
    "X_test = X_test_org.T/255\n",
    "\n",
    "y_train = np.array([y_train_org.T])\n",
    "print(y_train.shape)\n",
    "y_test = np.array([y_test_org.T])\n",
    "\n",
    "#training data for only the one-digit which will be used to test the single-digit classifier\n",
    "y_train_ones = y_train ==1\n",
    "y_test_ones = y_test ==1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Logistic Regression using one-vs-all classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The methodology of using binary classification in one-vs-all multiclassification is outlined briefly here: http://mlwiki.org/index.php/One-vs-All_Classification. A binary classifier using logistic regression will be trained separately for each digit. This means that the dataset will be modified where only zero's are a positive result (y=1) and all other digits are a negative result (y=0). The binary classifier will then be trained to identify zero's (y=1) resulting in a weights matrix (generally denoted W, but denoted W0 for the weights of the zero classifier) and the bias vector, b (also denoted b0 for the zero classifier). \n",
    "\n",
    "10 classifiers will be trained for each digit each with their own weights (W) and bias (b). Then, each example will be run through the 10 classifiers and the classifier that yields the greatest value, indicating the strongest classification, will be chosen. Through this way, the entire dataset can be used to classified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize():\n",
    "    '''\n",
    "    Description\n",
    "    This funciton initializes the parameters in the classifier with the W-weights initialized to small random\n",
    "        numbers and the bias term, b, intialized to zeros.\n",
    "    \n",
    "    Input\n",
    "    For a simple classifier, no inputs are needed because there is only a single W matrix and a single b-vector.\n",
    "    For a neural network, separate weight matricies and bias vectors need to be created for each layer.\n",
    "    \n",
    "    Output\n",
    "    W - the weight matrix\n",
    "    b - the bias vector\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    np.random.seed(2) #sets the seed of the random number generator\n",
    "    \n",
    "    W = np.random.randn(1, X_train.shape[0])*0.01\n",
    "    #the '[0]' term below is used to strip one set of square brackets off b so that its shape is consisent with db below\n",
    "    b = np.zeros((1,1))\n",
    "  \n",
    "    \n",
    "    #the assert statements are used to ensure the W and b matricies have the correct shape.\n",
    "    assert(W.shape == (1, X_train.shape[0]))\n",
    "    assert(b.shape == (1,1))\n",
    "        \n",
    "    return W, b\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this commmented command can be used to check the output of the initialize fuction\n",
    "#initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    '''\n",
    "    Description\n",
    "    The sigmoid function, which is used in logistic regression.\n",
    "    \n",
    "    Input\n",
    "    A value or np.array, x\n",
    "    \n",
    "    Ouput\n",
    "    The sigmoid of the input\n",
    "    '''\n",
    "    \n",
    "    sig = 1/(1+np.exp(-x))\n",
    "    \n",
    "    return sig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At large negative values, the sigmoid function outputs zero; at zero, the sigmoid outputs 0.5; and at large positive values, the sigmoid outputs one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.53978687e-05, 5.00000000e-01, 9.99954602e-01])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sig_test = np.array([-10,0,10])\n",
    "sigmoid(sig_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(W, b, X):\n",
    "    '''\n",
    "    Description\n",
    "    This function calculates the forward progagation of logistic regression. Here the activation function is\n",
    "        the sigmoid function\n",
    "    \n",
    "    Input\n",
    "    W - the weight matrix\n",
    "    b - the bias bector\n",
    "    X - the input data\n",
    "    \n",
    "    Output\n",
    "    Z - the linear multiplication of W and X added to the bias term, b\n",
    "    A - the activation value, which is the activation function computed on the Z-value\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    Z = np.dot(W, X) + b\n",
    "    A = sigmoid(Z)    \n",
    "        \n",
    "    return A, Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The propagation function which conducts the forward propagation step, computes the cost function, and updates the W and b terms using gradient descent. Gradient descent is done by calculating the derivative of the cost function with respect to the W and b terms. This derivative is calculated using the chain rule. \n",
    "\n",
    "The best concise description I can find for this is here: https://medium.com/technology-nineleaps/logistic-regression-gradient-descent-optimization-part-1-ed320325a67e - through this article doesn't derive the derivative calculations, dW (which is shorthand for $\\partial$C/$\\partial$W where C is the cost funcition) and db (which is also $\\partial$C/$\\partial$b)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def propagate(X, W, b, Y):\n",
    "    '''\n",
    "    Description:\n",
    "    Calls the forward_prop function top conduct forward propogation, calculates the cost function, and calculates\n",
    "        the derivatives of the cost function with respect to W and b to implement gradient descent to update W and b.\n",
    "    \n",
    "    Input:\n",
    "    X   - the training data\n",
    "    W   - the weights\n",
    "    \n",
    "\n",
    "    Output:\n",
    "    cost    - the cost function calculated\n",
    "    dW   - the gradient of the W-weight\n",
    "    db   - the gradient of the bias term, b\n",
    "    '''\n",
    "    \n",
    "\n",
    "    A, Z = forward_prop(W, b, X)\n",
    "    \n",
    "    m = A.shape[1]\n",
    "    \n",
    "    #calculating the cost function of logistic regressions\n",
    "    cost = -1/m * np.sum(Y * np.log(A) + (1 - Y) * np.log(1-A), axis=1) \n",
    "    \n",
    "    #calculates the gradients of W and b\n",
    "    dW = 2/m * np.dot(A - Y, X.T)\n",
    "    db = np.array([1/m * np.sum(A - Y, axis=1)])\n",
    "    #print(db.shape)\n",
    "    \n",
    "    assert(dW.shape == W.shape)\n",
    "    assert(db.shape == b.shape)\n",
    "\n",
    "            \n",
    "    return cost, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_digit_classifier(X_train, Y_train, learning_rate=0.1, iterations = 1000, print_cost = True):\n",
    "    '''\n",
    "    Description\n",
    "    Runs the logistic regression for a single digit\n",
    "\n",
    "    Inputs\n",
    "    X_train - the input features of the training data\n",
    "    Y_train - the labels of the digits in the training set\n",
    "    learning_rate - the learning rate used in gradient descent, set to 0.1 by default\n",
    "    iternations - the number of iterations of the gradient descent, set to 500 by default\n",
    "    print_cost - a binary label to see if the cost of of every 100 iterations will be printed\n",
    "    \n",
    "    Outputs\n",
    "    W - the final value of the W-weight optimized by gradient descent\n",
    "    b - the final value of the b bias term optimized by gradient descent\n",
    "    costs - a list of the costs calculated over the iterations\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    costs = []\n",
    "    W, b = initialize()\n",
    "    \n",
    "    for i in range(iterations):\n",
    "\n",
    "        cost, dW, db = propagate(X_train, W, b, Y_train)\n",
    "        \n",
    "        W = W - learning_rate * dW\n",
    "        b = b - learning_rate * db\n",
    "        \n",
    "        \n",
    "        if print_cost and i % 200 == 0:\n",
    "            print('Cost for ' + str(i) + '= ' + str(cost) ) \n",
    "        \n",
    "        if print_cost and i % 10 == 0:\n",
    "            costs.append(cost)\n",
    "            \n",
    "    if print_cost : \n",
    "        plt.plot(costs)\n",
    "        plt.ylabel('Costs')\n",
    "        plt.xlabel('Iterations (x10)')\n",
    "        plt.title(\"Learning rate is: \"+ str(learning_rate))\n",
    "        plt.show()\n",
    "    \n",
    "    return W, b, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost for 0= [0.6539679]\n",
      "Cost for 200= [0.04773371]\n",
      "Cost for 400= [0.04133387]\n",
      "Cost for 600= [0.03861992]\n",
      "Cost for 800= [0.03699522]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sd  = single_digit_classifier(X_train, y_train_ones)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the long tail of the cost functions exponential decrease, we can safely say that gradient descent has run through a sufficient number of iternations. If without seeing the long tail, we likely could increase the algorithm's performance by running gradient descent for more iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_digit_error(X,W,b, y):\n",
    "    '''\n",
    "    Description\n",
    "    Calculates the error of the predictions for the single digit classifier. In this function, the classifier is\n",
    "        seen to \"predict\" a certain value if the A-values are greater than 0.5, which is the classification threshold.\n",
    "        This will not be how the error is calculate or how prediction is calculated with multiple digits. \n",
    "    \n",
    "    Inputs\n",
    "    The usual suspects\n",
    "    \n",
    "    Ouput\n",
    "    The error term which is the average deviation of the predictions from the y-labels\n",
    "    '''\n",
    "    \n",
    "    A, Z = forward_prop(W,b,X)\n",
    "    \n",
    "    predictions = A>0.5*1.0\n",
    "    \n",
    "    m = predictions.shape[1]\n",
    "    \n",
    "    error = 1/m * np.sum(predictions!=y)\n",
    "    \n",
    "    return error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the error function for the single digit classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set error:  0.010833333333333334\n",
      "Test set error:  0.008\n"
     ]
    }
   ],
   "source": [
    "single_train_error = single_digit_error(X_train, sd[0], sd[1], y_train_ones)\n",
    "single_test_error = single_digit_error(X_test, sd[0], sd[1], y_test_ones)\n",
    "print('Training set error: ', single_train_error)\n",
    "print('Test set error: ', single_test_error)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method of making predictions in the single_digit_error calculator is likely inaccurate as the training set error and test set errors are too small when seen in the context of the performance of other learning algorithms on this dataset, as shown here: http://yann.lecun.com/exdb/mnist/. The lowest error on the dataset was 0.23 from using a 35 layer neural network. Granted, the error calculated above is only for classifying a single digit, while the 0.23 is for classifying all digits. One potential reason why our single digit classifer is doing so well could be do to the low frequency of positive events (y=1). There are around 6,500 ones in the dataset, which is roughly 10% of the dataset, which means that even if no learning occured and the algorithm always predicted a y=0, it would have a 10% error rate. \n",
    "\n",
    "I won't spend too much time disecting the reason for the low error rate, as simply classifying ones is only 1/10 of the stated mission. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-digit Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_digit_classifier(X_train, y_train, learning_rate):\n",
    "    '''\n",
    "    Description\n",
    "    This function calls the single_digit_classifier fuction on every number in the data set (0-9), which generates\n",
    "    W and b values for each digit. \n",
    "    \n",
    "    Input\n",
    "    X_train - the features in the training set\n",
    "    y_train - the labels of the features in the training set\n",
    "    learning_rate - the learing rate used in gradient descent - it is an input to the single_digit_classifier\n",
    "    \n",
    "    Output\n",
    "    parameters - a dictionary of the W and b parameters for each digit classifier with labels 'W0' for the \n",
    "        W-weight of the zero classifier and 'b0' for the b-term of the zero classifier.\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    parameters = {}\n",
    "#    cost_dic = {}\n",
    "    m = X_train.shape[1]\n",
    "    \n",
    "    for i in range(10):\n",
    "        y_train_temp = y_train ==i\n",
    "        W, b, costs = single_digit_classifier(X_train, y_train_temp, learning_rate, print_cost=False)\n",
    "        parameters['W_'+str(i)] = W\n",
    "        parameters['b_'+str(i)] = b\n",
    "#        costs['costs_'+str(i)] = costs\n",
    "\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the multi-digit classifier. The classifier takes almost 3 minutes to run, which doesn't bode well for parameter tuning. In future work, I may consider how to reduce the number of for-loops to try to speed up the computation time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minutes to run classifier:  6.717435685793559\n"
     ]
    }
   ],
   "source": [
    "time1 = time.time()\n",
    "multi_digit_parameters = multi_digit_classifier(X_train, y_train, learning_rate=0.1)\n",
    "time2 = time.time()\n",
    "print('Minutes to run classifier: ', (time2-time1)/60 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_digit_predict(parameters, X):\n",
    "    '''\n",
    "    Description\n",
    "    This function uses the parameters of the multi_digit_classifier to calculate the A-values for each single \n",
    "        digit classifier and takes the digit with the maximum A-value as the predicted digit.\n",
    "    \n",
    "    Input\n",
    "    parameters - the W and b values for each single digit classifier outputted from the multi_digit_classifier\n",
    "    X          - the the feature data that will be used along with the parameters to calculate the A-values \n",
    "    \n",
    "    Output\n",
    "    predictions - a list of the predicted numbers for each of the examples in the dataset, X\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    A_values, Z = forward_prop(parameters['W_'+str(0)], parameters['b_'+str(0)], X)\n",
    "    for i in range(1,10):\n",
    "        A_temp, Z = forward_prop(parameters['W_'+str(i)], parameters['b_'+str(i)], X)\n",
    "        A_values = np.append(A_values, A_temp, axis=0)\n",
    "    \n",
    "    predictions = []\n",
    "    for j in range(X.shape[1]):\n",
    "        predictions.append(np.argmax(A_values[:,j]))\n",
    "    \n",
    "    assert(len(predictions) == X.shape[1])\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A sample of the predictions:  [5, 0, 4, 1, 9, 2, 1, 3, 1, 4, 3, 5, 3, 6, 1]\n"
     ]
    }
   ],
   "source": [
    "multi_train_predictions = multi_digit_predict(multi_digit_parameters, X_train)\n",
    "multi_test_predictions = multi_digit_predict(multi_digit_parameters, X_test)\n",
    "\n",
    "print('A sample of the predictions: ',multi_train_predictions[0:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_digit_error(predictions, y):\n",
    "    '''\n",
    "    Description\n",
    "    This function calculates the error for the multi_digit_classifier my taking the average number of incorrrect\n",
    "    classifications of the y-labels in the training set.\n",
    "    \n",
    "    Inputs\n",
    "    predictions - the predictions of the digits in each image, the output the multi_digit_predict function\n",
    "    y - the labels of the training set data\n",
    "    \n",
    "    Outputs\n",
    "    error - the average number of incorrect classifications\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    m = len(predictions)\n",
    "    error = 1/m * np.sum(predictions!=y)\n",
    "    \n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-digit training error: 0.09856666666666668\n",
      "Multi-digit test error: 0.0921\n"
     ]
    }
   ],
   "source": [
    "multi_train_error = multi_digit_error(multi_train_predictions,y_train)\n",
    "multi_test_error = multi_digit_error(multi_test_predictions,y_test)\n",
    "print(\"Multi-digit training error: \"+ str(multi_train_error))\n",
    "print(\"Multi-digit test error: \"+ str(multi_test_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error on the training set is around 9.9% and is comparable to the 9.2% test set error. Both of these errors seem reasonable in the context of the performance of other learning algorithms as shown at: http://yann.lecun.com/exdb/mnist/. The error rate of a linear classifier is 12%. \n",
    "\n",
    "By comparing the training and test set errors, we can gain insight into the learning algorithm. If the training set error was small compared to the test set error, then the model would have high variance (overfitting). If the training set error was large compared to what we would expect the model to perform, then the model would have high bias (underfitting). Since the training and test set errors are very close, the model is does not have high variance, and since it is performing fairly well compared to the 12% error of a linear classifier, the model does not suffer from high bias, either. \n",
    "\n",
    "Adding regularization is a logical next step to improve the performance; however, since the model doesn't suffer from high variance, adding regularization is unlikely to improve the performance dramatically. Instead, it may be better to increase the model's complexity by turning it into a neural network and then add regularization, which will be be the next step in this exercise. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or instead, I may analzye the same problem using SVM's to buff up on their application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Neural Network Approach using Softmax in Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below will use a 3-layer neural network to analyze the handwritten digit dataset. I plan to rewrite the outline of the code below using Tensorflow to have a 3 layer neural network with two layers using a relu activation function and the final layer using a softmax activation function. Coming soon..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[784, 20, 9, 9]"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#defines the size of the neural network\n",
    "layers_dims = [X_train.shape[0], 20, 9, 9]\n",
    "layers_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_x:  784\n",
      "n_y:  10\n"
     ]
    }
   ],
   "source": [
    "n_x, n_y = 784, 10 #the input size and label size, respecitively\n",
    "print('n_x: ', n_x)\n",
    "print('n_y: ', n_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_placeholders(n_x, n_y):\n",
    "    '''\n",
    "    Description:\n",
    "    This creates the placeholders of the tensorflow session.\n",
    "    \n",
    "    Inputs:\n",
    "    n_x -- a scalar of the image size (784 pixels)\n",
    "    n_y -- a scalar of the number of classes (10 numbers)\n",
    "    \n",
    "    Outputs:\n",
    "    X -- a tf placeholder for the input data of shape [n_x, None] and data type float32\n",
    "    Y -- a tf placeholder for the input labels of shape [n_y, None] and data type float32\n",
    "    '''\n",
    "    \n",
    "    X = tf.placeholder(dtype=tf.float32, shape = [n_x, None], name='X')\n",
    "    Y = tf.placeholder(dtype=tf.float32, shape = [n_y, None], name='Y')\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X = Tensor(\"X:0\", shape=(784, ?), dtype=float32)\n",
      "Y = Tensor(\"Y:0\", shape=(10, ?), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "X, Y = create_placeholders(n_x, n_y)\n",
    "print (\"X = \" + str(X))\n",
    "print (\"Y = \" + str(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_initialize(layers_dims):\n",
    "    '''\n",
    "    Description: \n",
    "    Initializes the parameters in neural network based on the size of layers_dims where each value of layers_dims \n",
    "        specifies the number of neurons in each layer\n",
    "        \n",
    "    Input:\n",
    "    layers_dims -- a list that defins the number of layers and the number of hidden units in each layer \n",
    "        - shape (1 x number of layers + 1)\n",
    "        \n",
    "    Ouptut:\n",
    "    parameters -- a dictionary of the intialize parameters for the nueral network\n",
    "    \n",
    "    **Note: try comparing random normal and Xavier distribution for initialization\n",
    "    '''\n",
    "    \n",
    "    ld = layers_dims # an abbreviation for layers_dims\n",
    "    parameters = {}\n",
    "    \n",
    "    for l in range(1, len(layers_dims)):\n",
    "        parameters['W'+str(l)] = tf.get_variable('W'+str(l), [ld[l], ld[l-1]], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "        parameters['b'+str(l)] = tf.get_variable('b'+str(l), [ld[l], 1], initializer = tf.zeros_initializer())\n",
    "\n",
    "    return parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'W1:0' shape=(20, 784) dtype=float32_ref>\n",
      "<tf.Variable 'b1:0' shape=(20, 1) dtype=float32_ref>\n",
      "<tf.Variable 'W2:0' shape=(9, 20) dtype=float32_ref>\n",
      "<tf.Variable 'b2:0' shape=(9, 1) dtype=float32_ref>\n",
      "<tf.Variable 'W3:0' shape=(9, 9) dtype=float32_ref>\n",
      "<tf.Variable 'b3:0' shape=(9, 1) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess:\n",
    "    parameters = nn_initialize(layers_dims)\n",
    "    for l in range(1,len(layers_dims)):\n",
    "        print(parameters['W'+str(l)])\n",
    "        print(parameters['b'+str(l)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_forward_prop(X, parameters):\n",
    "    '''“\n",
    "    Description:\n",
    "    Performs the forward propagation steps of the neural network using the relu activiation function\n",
    "    \n",
    "    Inputs:\n",
    "    X -- the input data, of shape (784 x # examples)\n",
    "    parameters -- a dictionary of the parameters of the neural network, mainly W matricies and b vectors \n",
    "        where the weights and bias terms of the first layer have the keys 'W1' and 'b1', respectively.““\n",
    "    \n",
    "    Outputs:\n",
    "    Z_L -- the linear combination of the final layer, which will be fed into the tensorflow cost function,\n",
    "        of shape (# classes (10) x # examples)\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    \n",
    "    cache = {'A0':X} #dictionary of the hidden linear and activation layers where the first A0 is X, the input data\n",
    "    L = len(parameters)//2 # the number of layers in the neural network\n",
    "    \n",
    "    for l in range(1, L): #the loop stops at the L-1 layer because the final activiation layer\n",
    "        W_l = parameters['W'+str(l)]\n",
    "        b_l = parameters['b'+str(l)] #unpacking the W and b terms to make the linear combination more readable\n",
    "        A_prev = cache['A'+str(l-1)] #the previous activation layer value. For l=1, A_prev = A0 = X\n",
    "        cache['Z'+str(l)] = tf.add( tf.matmul(W_l, A_prev), b_l )\n",
    "        cache['A'+str(l)] = tf.nn.relu(cache['Z'+str(l)])\n",
    "    \n",
    "    cache['Z'+str(L)] = tf.add( tf.matmul(parameters['W'+str(L)], cache['A'+str(L-1)]), parameters['b'+str(L)] )\n",
    "    Z_L = cache['Z'+str(L)]\n",
    "    \n",
    "    return Z_L, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z_L = Tensor(\"Add_2:0\", shape=(9, ?), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    X, Y = create_placeholders(n_x, n_y)\n",
    "    parameters = nn_initialize(layers_dims)\n",
    "    Z_L, cache = nn_forward_prop(X, parameters)\n",
    "    print(\"Z_L = \" + str(Z_L))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(Z_L, Y):\n",
    "    '''\n",
    "    Description:\n",
    "    Creates a tensor of softmax cost function\n",
    "    \n",
    "    Inputs:\n",
    "    Z_L  --  the linear unit of the last layer in the neural network, of shape (# classes (10) x # examples)\n",
    "    Y    --  the input labels, of shape (10, # examples)\n",
    "    \n",
    "    Outputs:\n",
    "    cost -- the tensor of the cost function\n",
    "    '''\n",
    "    \n",
    "    #transposes the inputs to fit the needed dimensions of the softmax cost function\n",
    "    logits = tf.transpose(Z_L)\n",
    "    labels = tf.transpose(Y)\n",
    "    \n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = labels))\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost = Tensor(\"Mean:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    X, Y = create_placeholders(n_x, n_y)\n",
    "    parameters = nn_initialize(layers_dims)\n",
    "    Z_L, cache = nn_forward_prop(X, parameters)\n",
    "    cost = compute_cost(Z_L, Y)\n",
    "    print(\"cost = \" + str(cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(layers_dims, X_train, Y_train, X_test, Y_test, learning_rate = 0.0001, iterations=100, print_cost=True):\n",
    "    '''\n",
    "    Description:\n",
    "    The neural network optimization model\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "    iternations -- a scalar, nummber of iterations of the optimization algorithm\n",
    "    print_cost  -- a boolean, if True the algorithm will print the cost over a certain number of iterations\n",
    "    \n",
    "    \n",
    "    Outputs:\n",
    "    \n",
    "    Note: test performance using Adam optimization versus gradient descent\n",
    "    Note1: Incorporate mini-batches into optimization\n",
    "    '''\n",
    "    \n",
    "    #the code below was taken more directly from the Deeplearning.ai course Improving Neural Networks\n",
    "    #as I am still learning the ropes of tensorflow\n",
    "    \n",
    "    ops.reset_default_graph()           # to be able to rerun the model without overwriting tf variables\n",
    "    tf.set_random_seed(1)               # to keep consistent results\n",
    "    seed = 3                            # to keep consistent results\n",
    "    (n_x, m) = X_train.shape            # (n_x: input size, m : number of examples in the train set)\n",
    "    n_y = Y_train.shape[0]              # n_y : output size\n",
    "    costs = []                          # To keep track of the cost\n",
    "    \n",
    "    X, Y = create_placeholders(n_x, n_y)\n",
    "    \n",
    "    parameters = nn_initialize(layers_dims)\n",
    "    \n",
    "    Z_L, _ = nn_forward_prop(X, parameters)\n",
    "    \n",
    "    cost = compute_cost(Z_L, Y)\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        sess.run(init)\n",
    "        \n",
    "        for i in range(iterations):\n",
    "            \n",
    "            _ , iter_cost = sess.run([optimizer, cost], feed_dict={X:X_train, Y:Y_train})\n",
    "\n",
    "            if print_cost == True and i % 10 == 0:\n",
    "                print (\"Cost after iteration %i: %f\" % (i, iter_cost))\n",
    "            if print_cost == True and i % 10 == 0:\n",
    "                costs.append(iter_cost)\n",
    "                \n",
    "        \n",
    "        plt.plot(np.squeeze(costs))\n",
    "        plt.ylabel('cost')\n",
    "        plt.xlabel('iterations (per tens)')\n",
    "        plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "        plt.show()\n",
    "        \n",
    "        parameters = sess.run(parameters)\n",
    "        \n",
    "        correct_prediction = tf.equal(tf.argmax(Z_L), tf.argmax(Y))\n",
    "        \n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, 'float'))\n",
    "        \n",
    "        print('Train Accuracy: ', accuracy.eval({X:X_train, Y:Y_train} ) )\n",
    "        print('Test Accuracy: ', accuracy.eval({X:X_test, Y:Y_test} ) )\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 88.562668\n",
      "Cost after iteration 10: 88.546799\n",
      "Cost after iteration 20: 88.727905\n",
      "Cost after iteration 30: 89.198502\n",
      "Cost after iteration 40: 89.910614\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-261-b62c4d1cc685>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers_dims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-260-eae4c37dda19>\u001b[0m in \u001b[0;36mmodel\u001b[0;34m(layers_dims, X_train, Y_train, X_test, Y_test, learning_rate, iterations, print_cost)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[0m_\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0miter_cost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mY_train\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mprint_cost\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 950\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    951\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1171\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1173\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1174\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1350\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1354\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1355\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1356\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1357\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1339\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1340\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1341\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1427\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1428\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1429\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1431\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "parameters = model(layers_dims, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Old Neural Network Outline without Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    '''\n",
    "    Description:\n",
    "    A relu function\n",
    "    \n",
    "    Input:\n",
    "    x -- a numpy array\n",
    "    \n",
    "    Output:\n",
    "    the relu function\n",
    "    '''\n",
    "    \n",
    "    return np.maximum(x,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(X):\n",
    "    '''\n",
    "    Description:\n",
    "    The softmax function\n",
    "    \n",
    "    Input:\n",
    "    \n",
    "    \n",
    "    Output:\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    exps = np.exp(X)\n",
    "    \n",
    "    return np.exp(X)/np.sum(exps)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_initialize_old(layers_dims):\n",
    "    '''\n",
    "    Description: \n",
    "    This function initializes all of the parameters for the neural network using the number of layers and hidden\n",
    "        units definted in layers_dims\n",
    "    \n",
    "    Inputs:\n",
    "    layers_dims -- a vector of the number of hidden units in each layers where the first (zero-th) value is the \n",
    "        number of features, m, in the input values\n",
    "        \n",
    "    Outputs:\n",
    "    parameters -- a dictionary of initialized parameters for the neural network\n",
    "    \n",
    "    '''\n",
    "\n",
    "    np.random.seed(2) #sets the seed of the random number generator\n",
    "    parameters = {}\n",
    "    \n",
    "    #as a note - the W parameters are initialized as a product of a random number and root (2/previous layer size)\n",
    "    # the square root term provides a better initialization of the parameters\n",
    "    for l in range(1, len(layers_dims)):\n",
    "        parameters['W'+str(l)] = np.random.randn(layers_dims[l], layers_dims[l-1]) * np.sqrt(2 / layer_dims[l-1])\n",
    "        parameters['b'+str(l)] = np.zeros( (layers_dims[l], 1) )\n",
    "        \n",
    "        assert(parameters['W' + str(l)].shape == layers_dims[l], layers_dims[l-1])\n",
    "        assert(parameters['b'+str(l)]).shape == layers_dims[l],1 )\n",
    "        \n",
    "    assert( len(parameters) == 2*( len(layers_dims)-1 ) )\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 784)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param = nn_initialize(layers_dims)\n",
    "param['W1'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the cost function for a neural network classifying 9 digits? What is the final step of the neural network? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_forward_prop(parameters, X):\n",
    "    '''\n",
    "    Description:\n",
    "    The forward propagation step in the neural network. For an N-layer neural network, there will be N relu layers\n",
    "    \n",
    "    Input:\n",
    "    parameters -- a dictionary of parameters of the neural network with W1 indicating the W-weight for the first\n",
    "        layer of the neural network\n",
    "    X         -- the inputs to the neural network with shape (input size (m) x number of examples (n) )\n",
    "    \n",
    "    \n",
    "    Output:\n",
    "    A_L       -- the activation value of the final layer of the L-layer neural network\n",
    "    cache     -- a dictionary of the intermediate Z and A values\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    L = len(parameters) // 2 # the number of layers in the neural network\n",
    "    cache = {'A0': X_train}\n",
    "    \n",
    "    \n",
    "    for l in range(1, L+1):\n",
    "        Z = np.dot(parameters['W'+str(l)],cache['A'+str(l-1)] ) + parameters['b'+str(l)]\n",
    "        A = relu(Z)\n",
    "        cache['Z'+str(l)], cache['A'+str(l)] = Z, A\n",
    "        \n",
    "    A_L = A\n",
    "    \n",
    "#    assert (len(cache)+1 == len(parameters)//2 )\n",
    "    \n",
    "    return A_L, cache\n",
    "                              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.00057345, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.00150871, 0.00224729, 0.0021462 , ..., 0.00188633, 0.00103728,\n",
       "        0.00075334],\n",
       "       [0.00108241, 0.00308841, 0.00204568, ..., 0.        , 0.00222153,\n",
       "        0.00308861],\n",
       "       ...,\n",
       "       [0.00056204, 0.        , 0.        , ..., 0.00084801, 0.        ,\n",
       "        0.00449367],\n",
       "       [0.00577001, 0.00131677, 0.00381508, ..., 0.00138423, 0.00266134,\n",
       "        0.00168717],\n",
       "       [0.00240789, 0.00269189, 0.        , ..., 0.00173917, 0.00157735,\n",
       "        0.00152875]])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_L, cache = nn_forward_prop(param, X_train)\n",
    "cache['A2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_backward_prop(parameters,X, cache):\n",
    "    '''\n",
    "    Description:\n",
    "    The backward propagation step of the neural network.\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "    \n",
    "    Outputs:\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossEntropy_loss(X, y):\n",
    "    \n",
    "    \"\"\"\n",
    "    X is the output from fully connected layer (num_examples x num_classes)\n",
    "    y is labels (num_examples x 1)\n",
    "    \tNote that y is not one-hot encoded vector. \n",
    "    \tIt can be computed as y.argmax(axis=1) from one-hot encoded vectors of labels if required.\n",
    "    \"\"\"\n",
    "    m = y.shape[0]\n",
    "    p = softmax(X)\n",
    "    # We use multidimensional array indexing to extract \n",
    "    # softmax probability of the correct label for each sample.\n",
    "    # Refer to https://docs.scipy.org/doc/numpy/user/basics.indexing.html#indexing-multi-dimensional-arrays for understanding multidimensional array indexing.\n",
    "    log_likelihood = -np.log(p[range(m),y])\n",
    "    loss = np.sum(log_likelihood) / m\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_predict(A_L):\n",
    "    '''\n",
    "    Description:\n",
    "    Takes in the final layer of the neural network and takes the highest value in that layer of the prediction\n",
    "    \n",
    "    \n",
    "    '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratch work - Tensorflow tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "coefficients = np.array([[1.], [-20.], [10.]])\n",
    "\n",
    "w = tf.Variable(0, dtype = tf.float32)\n",
    "x = tf.placeholder(tf.float32, [3,1])\n",
    "#cost = tf.add( tf.add(w**2, tf.multiply(-10.,w) ), 25)\n",
    "#cost = w**2 - 10*w + 25\n",
    "cost = x[0][0]*w**2 + x[1][0]*w + x[2][0]\n",
    "train = tf.train.GradientDescentOptimizer(0.01).minimize(cost)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "#session = tf.Session()\n",
    "#session.run(init)\n",
    "#print(session.run(w))\n",
    "\n",
    "with tf.Session() as session:\n",
    "    session.run(init)\n",
    "    print(session.run(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19999999\n"
     ]
    }
   ],
   "source": [
    "session.run(train, feed_dict = {x:coefficients})\n",
    "print(session.run(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.999977\n"
     ]
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "    session.run(train, feed_dict={x:coefficients})\n",
    "print(session.run(w))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
