{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Handwritten Digit Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook uses logistic regression as a one-vs-all multi-class classification of the 0 through 9 handwritten digits in the MNIST dataset. A detailed description of the dataset and a catolog of previous analysis on the datset can be found here: http://yann.lecun.com/exdb/mnist/. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideas for improvement\n",
    " - tune alpha (and lambda)\n",
    " - add regularization\n",
    " - Use other algorithms like\n",
    "     - apply neural networks w/ softmax\n",
    "     - apply SVM's\n",
    "     - apply kNN\n",
    "     - apply decision trees\n",
    " - implement Adam optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from mlxtend.data import loadlocal_mnist\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset was downloaded from http://yann.lecun.com/exdb/mnist/ and then unzipped. The data was then processed using the mlxtend.data library outlined here:  https://rasbt.github.io/mlxtend/user_guide/data/loadlocal_mnist/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_org, y_train_org = loadlocal_mnist(\n",
    "        images_path='train-images-idx3-ubyte', \n",
    "        labels_path='train-labels-idx1-ubyte')\n",
    "\n",
    "X_test_org, y_test_org = loadlocal_mnist(\n",
    "        images_path='t10k-images-idx3-ubyte', \n",
    "        labels_path='t10k-labels-idx1-ubyte')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The images of each digit are made up for 28 x 28 pixels (784 total), which have been reformatted into a single vector of length 784. There are 60,000 images in the training set and 10,000 in the test set. The y_train and y_test are oddly shaped, which is a concern, though it doesn't effect the analyis in the end. I tried to remedy this using the np.squeeze function, but it didn't work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train dimensions: 60000 x 784\n",
      "y_train dimensions:(60000,)\n",
      "X_test dimensionss: 10000 x 784\n",
      "y_test dimensions:(10000,)\n",
      "Example of X_train: [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "Example of y_train: [5 0 4 1 9 2 1 3 1 4 3 5 3 6 1]\n"
     ]
    }
   ],
   "source": [
    "print('X_train dimensions: %s x %s' % (X_train_org.shape[0], X_train_org.shape[1]))\n",
    "print('y_train dimensions:'+ str(y_train_org.shape )) \n",
    "print('X_test dimensionss: %s x %s' % (X_test_org.shape[0], X_test_org.shape[1]))\n",
    "print('y_test dimensions:'+ str(y_test_org.shape )) \n",
    "print('Example of X_train:', X_train_org[0:15,0:15])\n",
    "print('Example of y_train:', y_train_org[0:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are images of what a given example of the handwritten digits look like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x116aea550>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "example_number = 2\n",
    "img = X_train_org[example_number,:].reshape(28,28)\n",
    "plt.imshow(img, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is currently formatted with the rows being the nummber of examples, (m=60,000), and the columns being the features, (n=784). The analysis I will perform has the data transposed, with size n x m, which will be done below. Additionally, I have scaled the data to be within zero and 1 by dividing by 255, as the pixel values are 8-bit integers. \n",
    "I remedied the missing column values in y_train and y_test with by putting them inside a np.array and transposed the y-values as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 60000)\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train_org.T/255\n",
    "X_test = X_test_org.T/255\n",
    "\n",
    "y_train = np.array([y_train_org.T])\n",
    "print(y_train.shape)\n",
    "y_test = np.array([y_test_org.T])\n",
    "\n",
    "#training data for only the one-digit which will be used to test the single-digit classifier\n",
    "y_train_ones = y_train ==1\n",
    "y_test_ones = y_test ==1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The methodology of using binary classification in one-vs-all multiclassification is outlined briefly here: http://mlwiki.org/index.php/One-vs-All_Classification. A binary classifier using logistic regression will be trained separately for each digit. This means that the dataset will be modified where only zero's are a positive result (y=1) and all other digits are a negative result (y=0). The binary classifier will then be trained to identify zero's (y=1) resulting in a weights matrix (generally denoted W, but denoted W0 for the weights of the zero classifier) and the bias vector, b (also denoted b0 for the zero classifier). \n",
    "\n",
    "10 classifiers will be trained for each digit each with their own weights (W) and bias (b). Then, each example will be run through the 10 classifiers and the classifier that yields the greatest value, indicating the strongest classification, will be chosen. Through this way, the entire dataset can be used to classified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize():\n",
    "    '''\n",
    "    Description\n",
    "    This funciton initializes the parameters in the classifier with the W-weights initialized to small random\n",
    "        numbers and the bias term, b, intialized to zeros.\n",
    "    \n",
    "    Input\n",
    "    For a simple classifier, no inputs are needed because there is only a single W matrix and a single b-vector.\n",
    "    For a neural network, separate weight matricies and bias vectors need to be created for each layer.\n",
    "    \n",
    "    Output\n",
    "    W - the weight matrix\n",
    "    b - the bias vector\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    np.random.seed(2) #sets the seed of the random number generator\n",
    "    \n",
    "    W = np.random.randn(1, X_train.shape[0])*0.01\n",
    "    #the '[0]' term below is used to strip one set of square brackets off b so that its shape is consisent with db below\n",
    "    b = np.zeros((1,1))\n",
    "  \n",
    "    \n",
    "    #the assert statements are used to ensure the W and b matricies have the correct shape.\n",
    "    assert(W.shape == (1, X_train.shape[0]))\n",
    "    assert(b.shape == (1,1))\n",
    "        \n",
    "    return W, b\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this commmented command can be used to check the output of the initialize fuction\n",
    "#initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    '''\n",
    "    Description\n",
    "    The sigmoid function, which is used in logistic regression.\n",
    "    \n",
    "    Input\n",
    "    A value or np.array, x\n",
    "    \n",
    "    Ouput\n",
    "    The sigmoid of the input\n",
    "    '''\n",
    "    \n",
    "    sig = 1/(1+np.exp(-x))\n",
    "    \n",
    "    return sig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At large negative values, the sigmoid function outputs zero; at zero, the sigmoid outputs 0.5; and at large positive values, the sigmoid outputs one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.53978687e-05, 5.00000000e-01, 9.99954602e-01])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sig_test = np.array([-10,0,10])\n",
    "sigmoid(sig_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(W, b, X):\n",
    "    '''\n",
    "    Description\n",
    "    This function calculates the forward progagation of logistic regression. Here the activation function is\n",
    "        the sigmoid function\n",
    "    \n",
    "    Input\n",
    "    W - the weight matrix\n",
    "    b - the bias bector\n",
    "    X - the input data\n",
    "    \n",
    "    Output\n",
    "    Z - the linear multiplication of W and X added to the bias term, b\n",
    "    A - the activation value, which is the activation function computed on the Z-value\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    Z = np.dot(W, X) + b\n",
    "    A = sigmoid(Z)    \n",
    "        \n",
    "    return A, Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The propagation function which conducts the forward propagation step, computes the cost function, and updates the W and b terms using gradient descent. Gradient descent is done by calculating the derivative of the cost function with respect to the W and b terms. This derivative is calculated using the chain rule. \n",
    "\n",
    "The best concise description I can find for this is here: https://medium.com/technology-nineleaps/logistic-regression-gradient-descent-optimization-part-1-ed320325a67e - through this article doesn't derive the derivative calculations, dW (which is shorthand for $\\partial$C/$\\partial$W where C is the cost funcition) and db (which is also $\\partial$C/$\\partial$b)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def propagate(X, W, b, Y):\n",
    "    '''\n",
    "    Description:\n",
    "    Calls the forward_prop function top conduct forward propogation, calculates the cost function, and calculates\n",
    "        the derivatives of the cost function with respect to W and b to implement gradient descent to update W and b.\n",
    "    \n",
    "    Input:\n",
    "    X   - the training data\n",
    "    W   - the weights\n",
    "    \n",
    "\n",
    "    Output:\n",
    "    cost    - the cost function calculated\n",
    "    dW   - the gradient of the W-weight\n",
    "    db   - the gradient of the bias term, b\n",
    "    '''\n",
    "    \n",
    "\n",
    "    A, Z = forward_prop(W, b, X)\n",
    "    \n",
    "    m = A.shape[1]\n",
    "    \n",
    "    #calculating the cost function of logistic regressions\n",
    "    cost = -1/m * np.sum(Y * np.log(A) + (1 - Y) * np.log(1-A), axis=1) \n",
    "    \n",
    "    #calculates the gradients of W and b\n",
    "    dW = 2/m * np.dot(A - Y, X.T)\n",
    "    db = np.array([1/m * np.sum(A - Y, axis=1)])\n",
    "    #print(db.shape)\n",
    "    \n",
    "    assert(dW.shape == W.shape)\n",
    "    assert(db.shape == b.shape)\n",
    "\n",
    "            \n",
    "    return cost, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_digit_classifier(X_train, Y_train, learning_rate=0.1, iterations = 1000, print_cost = True):\n",
    "    '''\n",
    "    Description\n",
    "    Runs the logistic regression for a single digit\n",
    "\n",
    "    Inputs\n",
    "    X_train - the input features of the training data\n",
    "    Y_train - the labels of the digits in the training set\n",
    "    learning_rate - the learning rate used in gradient descent, set to 0.1 by default\n",
    "    iternations - the number of iterations of the gradient descent, set to 500 by default\n",
    "    print_cost - a binary label to see if the cost of of every 100 iterations will be printed\n",
    "    \n",
    "    Outputs\n",
    "    W - the final value of the W-weight optimized by gradient descent\n",
    "    b - the final value of the b bias term optimized by gradient descent\n",
    "    costs - a list of the costs calculated over the iterations\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    costs = []\n",
    "    W, b = initialize()\n",
    "    \n",
    "    for i in range(iterations):\n",
    "\n",
    "        cost, dW, db = propagate(X_train, W, b, Y_train)\n",
    "        \n",
    "        W = W - learning_rate * dW\n",
    "        b = b - learning_rate * db\n",
    "        \n",
    "        \n",
    "        if print_cost and i % 200 == 0:\n",
    "            print('Cost for ' + str(i) + '= ' + str(cost) ) \n",
    "        \n",
    "        if print_cost and i % 10 == 0:\n",
    "            costs.append(cost)\n",
    "            \n",
    "    if print_cost : \n",
    "        plt.plot(costs)\n",
    "        plt.ylabel('Costs')\n",
    "        plt.xlabel('Iterations (x10)')\n",
    "        plt.title(\"Learning rate is: \"+ str(learning_rate))\n",
    "        plt.show()\n",
    "    \n",
    "    return W, b, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost for 0= [0.6539679]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-133-08168faa1dab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msd\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0msingle_digit_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_ones\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-132-36775494f63c>\u001b[0m in \u001b[0;36msingle_digit_classifier\u001b[0;34m(X_train, Y_train, learning_rate, iterations, print_cost)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mcost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpropagate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mW\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdW\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-1d13a530fd98>\u001b[0m in \u001b[0;36mpropagate\u001b[0;34m(X, W, b, Y)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;31m#calculates the gradients of W and b\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mdW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mm\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0mdb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mm\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;31m#print(db.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sd  = single_digit_classifier(X_train, y_train_ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_digit_error(X,W,b, y):\n",
    "    '''\n",
    "    Description\n",
    "    Calculates the error of the predictions for the single digit classifier. In this function, the classifier is\n",
    "        seen to \"predict\" a certain value if the A-values are greater than 0.5, which is the classification threshold.\n",
    "        This will not be how the error is calculate or how prediction is calculated with multiple digits. \n",
    "    \n",
    "    Inputs\n",
    "    The usual suspects\n",
    "    \n",
    "    Ouput\n",
    "    The error term which is the average deviation of the predictions from the y-labels\n",
    "    '''\n",
    "    \n",
    "    A, Z = forward_prop(W,b,X)\n",
    "    \n",
    "    predictions = A>0.5*1.0\n",
    "    \n",
    "    m = predictions.shape[1]\n",
    "    \n",
    "    error = 1/m * np.sum(predictions!=y)\n",
    "    \n",
    "    return error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the error function for the single digit classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set error:  0.010833333333333334\n",
      "Test set error:  0.008\n"
     ]
    }
   ],
   "source": [
    "single_train_error = single_digit_error(X_train, sd[0], sd[1], y_train_ones)\n",
    "single_test_error = single_digit_error(X_test, sd[0], sd[1], y_test_ones)\n",
    "print('Training set error: ', single_train_error)\n",
    "print('Test set error: ', single_test_error)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method of making predictions in the single_digit_error calculator is likely inaccurate as the training set error and test set errors are too small when seen in the context of the performance of other learning algorithms on this dataset, as shown here: http://yann.lecun.com/exdb/mnist/. The lowest error on the dataset was 0.23 from using a 35 layer neural network. Granted, the error calculated above is only for classifying a single digit, while the 0.23 is for classifying all digits. One potential reason why our single digit classifer is doing so well could be do to the low frequency of positive events (y=1). There are around 6,500 ones in the dataset, which is roughly 10% of the dataset, which means that even if no learning occured and the algorithm always predicted a y=0, it would have a 10% error rate. \n",
    "\n",
    "I won't spend too much time disecting the reason for the low error rate, as simply classifying ones is only 1/10 of the stated mission. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-digit Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_digit_classifier(X_train, y_train, learning_rate):\n",
    "    '''\n",
    "    Description\n",
    "    This function calls the single_digit_classifier fuction on every number in the data set (0-9), which generates\n",
    "    W and b values for each digit. \n",
    "    \n",
    "    Input\n",
    "    X_train - the features in the training set\n",
    "    y_train - the labels of the features in the training set\n",
    "    learning_rate - the learing rate used in gradient descent - it is an input to the single_digit_classifier\n",
    "    \n",
    "    Output\n",
    "    parameters - a dictionary of the W and b parameters for each digit classifier with labels 'W0' for the \n",
    "        W-weight of the zero classifier and 'b0' for the b-term of the zero classifier.\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    parameters = {}\n",
    "    cost_dic = {}\n",
    "    m = X_train.shape[1]\n",
    "    \n",
    "    for i in range(10):\n",
    "        y_train_temp = y_train ==i\n",
    "        W, b, costs = single_digit_classifier(X_train, y_train_temp, learning_rate, print_cost=True)\n",
    "        parameters['W_'+str(i)] = W\n",
    "        parameters['b_'+str(i)] = b\n",
    "        costs['costs_'+str(i)] = costs\n",
    "\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the multi-digit classifier. The classifier takes almost 3 minutes to run, which doesn't bode well for parameter tuning. In future work, I will consider how to reduce the number of for-loops to try to speed up the computation time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run classifier:  193.93325185775757\n"
     ]
    }
   ],
   "source": [
    "time1 = time.time()\n",
    "multi_digit_parameters = multi_digit_classifier(X_train, y_train, learning_rate=0.1)\n",
    "time2 = time.time()\n",
    "print('Minutes to run classifier: ', (time2-time1)/60 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_digit_predict(parameters, X):\n",
    "    '''\n",
    "    Description\n",
    "    This function uses the parameters of the multi_digit_classifier to calculate the A-values for each single \n",
    "        digit classifier and takes the digit with the maximum A-value as the predicted digit.\n",
    "    \n",
    "    Input\n",
    "    parameters - the W and b values for each single digit classifier outputted from the multi_digit_classifier\n",
    "    X          - the the feature data that will be used along with the parameters to calculate the A-values \n",
    "    \n",
    "    Output\n",
    "    predictions - a list of the predicted numbers for each of the examples in the dataset, X\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    A_values, Z = forward_prop(parameters['W_'+str(0)], parameters['b_'+str(0)], X)\n",
    "    for i in range(1,10):\n",
    "        A_temp, Z = forward_prop(parameters['W_'+str(i)], parameters['b_'+str(i)], X)\n",
    "        A_values = np.append(A_values, A_temp, axis=0)\n",
    "    \n",
    "    predictions = []\n",
    "    for j in range(X.shape[1]):\n",
    "        predictions.append(np.argmax(A_values[:,j]))\n",
    "    \n",
    "    assert(len(predictions) == X.shape[1])\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A sample of the predictions:  [5, 0, 4, 1, 9, 2, 1, 3, 1, 4, 3, 5, 3, 6, 1]\n"
     ]
    }
   ],
   "source": [
    "multi_train_predictions = multi_digit_predict(multi_digit_parameters, X_train)\n",
    "multi_test_predictions = multi_digit_predict(multi_digit_parameters, X_test)\n",
    "\n",
    "print('A sample of the predictions: ',all_train_predictions[0:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_digit_error(predictions, y):\n",
    "    '''\n",
    "    Description\n",
    "    This function calculates the error for the multi_digit_classifier my taking the average number of incorrrect\n",
    "    classifications of the y-labels in the training set.\n",
    "    \n",
    "    Inputs\n",
    "    predictions - the predictions of the digits in each image, the output the multi_digit_predict function\n",
    "    y - the labels of the training set data\n",
    "    \n",
    "    Outputs\n",
    "    error - the average number of incorrect classifications\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    m = len(predictions)\n",
    "    error = 1/m * np.sum(predictions!=y)\n",
    "    \n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'multi_train_predictions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-aa15a025b02c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmulti_train_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmulti_digit_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmulti_train_predictions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmulti_test_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmulti_digit_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmulti_test_predictions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Multi-digit training error: \"\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_all_error\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Multi-digit test error: \"\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_all_error\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'multi_train_predictions' is not defined"
     ]
    }
   ],
   "source": [
    "multi_train_error = multi_digit_error(multi_train_predictions,y_train)\n",
    "multi_test_error = multi_digit_error(multi_test_predictions,y_test)\n",
    "print(\"Multi-digit training error: \"+ str(train_all_error))\n",
    "print(\"Multi-digit test error: \"+ str(test_all_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error on the training set is around 10.6% and is comparable to the 10% test set error. Both of these errors seem reasonable in the context of the performance of other learning algorithms as shown at: http://yann.lecun.com/exdb/mnist/. The error rate of a linear classifier is 12%. \n",
    "\n",
    "By comparing the training and test set errors, we can gain insight into the learning algorithm. If the training set error was small compared to the test set error, then the model would have high variance (overfitting). If the training set error was large compared to what we would expect the model to perform, then the model would have high bias (underfitting). Since the training and test set errors are very close, the model is does not have high variance, and since it is performing fairly well compared to the 12% error of a linear classifier, the model does not suffer from high bias, either. \n",
    "\n",
    "Adding regularization is a logical next step to improve the performance; however, since the model doesn't suffer from high variance, adding regularization is unlikely to improve the performance dramatically. Instead, it may be better to increase the model's complexity by turning it into a neural network and then add regularization, which will be be the next step in this exercise. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or instead, I may analzye the same problem using SVM's to buff up on their application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Neural Network Approach using Softmax in Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below will use a shallow neural network to analyze the handwritten digit dataset. I plan to rewrite the code below using Tensorflow to have a 3 layer neural network with two layers using a relu activation function and the final layer using a softmax activation function. Coming soon..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_dims = [X_train.shape[0], 20, 9, 9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    '''\n",
    "    Description:\n",
    "    A relu function\n",
    "    \n",
    "    Input:\n",
    "    x -- a numpy array\n",
    "    \n",
    "    Output:\n",
    "    the relu function\n",
    "    '''\n",
    "    \n",
    "    return np.maximum(x,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(X):\n",
    "    '''\n",
    "    Description:\n",
    "    The softmax function\n",
    "    \n",
    "    Input:\n",
    "    \n",
    "    \n",
    "    Output:\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    exps = np.exp(X)\n",
    "    \n",
    "    return np.exp(X)/np.sum(exps)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_initialize(layers_dims):\n",
    "    '''\n",
    "    Description: \n",
    "    This function initializes all of the parameters for the neural network using the number of layers and hidden\n",
    "        units definted in layers_dims\n",
    "    \n",
    "    Inputs:\n",
    "    layers_dims -- a vector of the number of hidden units in each layers where the first (zero-th) value is the \n",
    "        number of features, m, in the input values\n",
    "        \n",
    "    Outputs:\n",
    "    parameters -- a dictionary of initialized parameters for the neural network\n",
    "    \n",
    "    '''\n",
    "\n",
    "    np.random.seed(2) #sets the seed of the random number generator\n",
    "    parameters = {}\n",
    "    \n",
    "    #as a note - the W parameters are initialized as a product of a random number and root (2/previous layer size)\n",
    "    # the square root term provides a better initialization of the parameters\n",
    "    for l in range(1, len(layers_dims)):\n",
    "        parameters['W'+str(l)] = np.random.randn(layers_dims[l], layers_dims[l-1]) * np.sqrt(2 / layer_dims[l-1])\n",
    "        parameters['b'+str(l)] = np.zeros( (layers_dims[l], 1) )\n",
    "        \n",
    "        assert(parameters['W' + str(l)].shape == layers_dims[l], layers_dims[l-1])\n",
    "        assert(parameters['b'+str(l)]).shape == layers_dims[l],1 )\n",
    "        \n",
    "    assert( len(parameters) == 2*( len(layers_dims)-1 ) )\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 784)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param = nn_initialize(layers_dims)\n",
    "param['W1'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the cost function for a neural network classifying 9 digits? What is the final step of the neural network? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_forward_prop(parameters, X):\n",
    "    '''\n",
    "    Description:\n",
    "    The forward propagation step in the neural network. For an N-layer neural network, there will be N relu layers\n",
    "    \n",
    "    Input:\n",
    "    parameters -- a dictionary of parameters of the neural network with W1 indicating the W-weight for the first\n",
    "        layer of the neural network\n",
    "    X         -- the inputs to the neural network with shape (input size (m) x number of examples (n) )\n",
    "    \n",
    "    \n",
    "    Output:\n",
    "    A_L       -- the activation value of the final layer of the L-layer neural network\n",
    "    cache     -- a dictionary of the intermediate Z and A values\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    L = len(parameters) // 2 # the number of layers in the neural network\n",
    "    cache = {'A0': X_train}\n",
    "    \n",
    "    \n",
    "    for l in range(1, L+1):\n",
    "        Z = np.dot(parameters['W'+str(l)],cache['A'+str(l-1)] ) + parameters['b'+str(l)]\n",
    "        A = relu(Z)\n",
    "        cache['Z'+str(l)], cache['A'+str(l)] = Z, A\n",
    "        \n",
    "    A_L = A\n",
    "    \n",
    "#    assert (len(cache)+1 == len(parameters)//2 )\n",
    "    \n",
    "    return A_L, cache\n",
    "                              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.00057345, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.00150871, 0.00224729, 0.0021462 , ..., 0.00188633, 0.00103728,\n",
       "        0.00075334],\n",
       "       [0.00108241, 0.00308841, 0.00204568, ..., 0.        , 0.00222153,\n",
       "        0.00308861],\n",
       "       ...,\n",
       "       [0.00056204, 0.        , 0.        , ..., 0.00084801, 0.        ,\n",
       "        0.00449367],\n",
       "       [0.00577001, 0.00131677, 0.00381508, ..., 0.00138423, 0.00266134,\n",
       "        0.00168717],\n",
       "       [0.00240789, 0.00269189, 0.        , ..., 0.00173917, 0.00157735,\n",
       "        0.00152875]])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_L, cache = nn_forward_prop(param, X_train)\n",
    "cache['A2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_backward_prop(parameters,X, cache):\n",
    "    '''\n",
    "    Description:\n",
    "    The backward propagation step of the neural network.\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "    \n",
    "    Outputs:\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossEntropy_loss(X, y):\n",
    "    \n",
    "    \"\"\"\n",
    "    X is the output from fully connected layer (num_examples x num_classes)\n",
    "    y is labels (num_examples x 1)\n",
    "    \tNote that y is not one-hot encoded vector. \n",
    "    \tIt can be computed as y.argmax(axis=1) from one-hot encoded vectors of labels if required.\n",
    "    \"\"\"\n",
    "    m = y.shape[0]\n",
    "    p = softmax(X)\n",
    "    # We use multidimensional array indexing to extract \n",
    "    # softmax probability of the correct label for each sample.\n",
    "    # Refer to https://docs.scipy.org/doc/numpy/user/basics.indexing.html#indexing-multi-dimensional-arrays for understanding multidimensional array indexing.\n",
    "    log_likelihood = -np.log(p[range(m),y])\n",
    "    loss = np.sum(log_likelihood) / m\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_predict(A_L):\n",
    "    '''\n",
    "    Description:\n",
    "    Takes in the final layer of the neural network and takes the highest value in that layer of the prediction\n",
    "    \n",
    "    \n",
    "    '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "coefficients = np.array([[1.], [-20.], [10.]])\n",
    "\n",
    "w = tf.Variable(0, dtype = tf.float32)\n",
    "x = tf.placeholder(tf.float32, [3,1])\n",
    "#cost = tf.add( tf.add(w**2, tf.multiply(-10.,w) ), 25)\n",
    "#cost = w**2 - 10*w + 25\n",
    "cost = x[0][0]*w**2 + x[1][0]*w + x[2][0]\n",
    "train = tf.train.GradientDescentOptimizer(0.01).minimize(cost)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "#session = tf.Session()\n",
    "#session.run(init)\n",
    "#print(session.run(w))\n",
    "\n",
    "with tf.Session() as session:\n",
    "    session.run(init)\n",
    "    print(session.run(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19999999\n"
     ]
    }
   ],
   "source": [
    "session.run(train, feed_dict = {x:coefficients})\n",
    "print(session.run(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.999977\n"
     ]
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "    session.run(train, feed_dict={x:coefficients})\n",
    "print(session.run(w))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
