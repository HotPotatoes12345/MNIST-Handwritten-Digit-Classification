{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook creates a basic neural network of arbitrary layer size to recognize digits in the MNIST dataset.\n",
    "\n",
    "\n",
    "Data Source: http://yann.lecun.com/exdb/mnist/\n",
    "\n",
    "Note: I give credit to Andrew Ng course on Coursera (at https://www.deeplearning.ai/), which is where I have learned to build neural networks. The structure of this notebook is similiar to the notebook's used in the course, but I have not copied anything from the course's notebooks (in accordance with the course license). The similarity with those notebooks is because that is the structure I learned from. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideas for improvement\n",
    " - change cost function to output 0 if the prediction is correct, or 1 if it is wrong\n",
    " - standardize the dataset\n",
    " - create more detailed introduction to the data\n",
    " \n",
    " \n",
    " Misteps\n",
    " - didn't include the 1/m factor in cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the data sets into python based on the method described here - https://rasbt.github.io/mlxtend/user_guide/data/loadlocal_mnist/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train dims: 784 x 60000\n",
      "y_train:(60000,)\n",
      "X_test dims: 784 x 10000\n",
      "y_test:(10000,)\n",
      "\n",
      "1st row 2\n"
     ]
    }
   ],
   "source": [
    "from mlxtend.data import loadlocal_mnist\n",
    "\n",
    "X_train, y_train = loadlocal_mnist(\n",
    "        images_path='train-images-idx3-ubyte', \n",
    "        labels_path='train-labels-idx1-ubyte')\n",
    "\n",
    "X_test, y_test = loadlocal_mnist(\n",
    "        images_path='t10k-images-idx3-ubyte', \n",
    "        labels_path='t10k-labels-idx1-ubyte')\n",
    "\n",
    "#transposing the data so that the dimensions are features (784 pixels) x examples (60,000 images in training set)\n",
    "X_train = X_train.T\n",
    "X_test = X_test.T\n",
    "y_train = np.squeeze(y_train).T\n",
    "y_test = np.squeeze(y_test).T\n",
    "\n",
    "print('X_train dims: %s x %s' % (X_train.shape[0], X_train.shape[1]))\n",
    "print('y_train:'+ str(y_train.shape )) \n",
    "print('X_test dims: %s x %s' % (X_test.shape[0], X_test.shape[1]))\n",
    "print('y_test:'+ str(y_test.shape )) \n",
    "\n",
    "print('\\n1st row', y_train[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 2. 5. 1. 2. 2. 3. 2. 4. 3. 2. 8. 3. 2. 5.]\n",
      "[5 0 4 1 9 2 1 3 1 4 3 5 3 6 1]\n",
      "[[14.4951]]\n"
     ]
    }
   ],
   "source": [
    "y1 = y_train-0\n",
    "y1.shape\n",
    "y2= m[0]-y_train\n",
    "print(m[0][0,0:15])\n",
    "print(y_train[0:15])\n",
    "print(1/m[0].shape[1]*np.dot(y2,y2.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize(layers_dims):\n",
    "    '''\n",
    "    Description\n",
    "    This funciton initializes the parameters in the neural network with the W-weights initialized to small random\n",
    "        numbers and the bias terms, b, intialized to zero\n",
    "    \n",
    "    Input\n",
    "    layers_dim - the size of the neural network layers with the zero-th element being the number of examples\n",
    "                - shape (1 x number of layers = L )\n",
    "    \n",
    "    Output\n",
    "    parameters - a dictionary of the initialized parameters \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    np.random.seed(2) #sets the seed of the random number generator\n",
    "    parameters = {}\n",
    "\n",
    "    for l in range(1,len(layers_dims)):\n",
    "        parameters['W' + str(l)] = np.random.randn(layers_dims[l], layers_dims[l-1]) * 0.1\n",
    "        parameters['b' + str(l)] = np.zeros((layers_dims[l],1))\n",
    "        \n",
    "    return parameters\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = initialize([X_train.shape[0],9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 784)"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters['W1'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_linear_activation(A_prev, W_curr, b_curr, activation = 'linear'):\n",
    "    '''\n",
    "    Description\n",
    "    Calculates the linear combination of a single layer in the network as well as the activation layer\n",
    "    \n",
    "    Input\n",
    "    A_prev - the activation values of the previous layer in the neural network -\n",
    "    W_curr - the W parameter for the current layer\n",
    "    b_curr - the b parameter in the current layer\n",
    "    activation - determines the activation function performed on the linear combination Z_curr\n",
    "    \n",
    "    Output\n",
    "    cache - a cache that stores the linear combination Z_curr and activation values A_curr\n",
    "    A_curr - the sum of the linear combination of the A_prev and W_curr values and the b_curr vector\n",
    "    '''\n",
    "    \n",
    "    Z_curr = np.dot(W_curr, A_prev) + b_curr\n",
    "    \n",
    "    if activation == 'linear':\n",
    "        A_curr = Z_curr\n",
    "        \n",
    "    elif activation == 'relu':\n",
    "        pass\n",
    "    \n",
    "    cache = [Z_curr, A_curr]\n",
    "    \n",
    "    return A_curr, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(X_train, parameters, layers_dims):\n",
    "    '''\n",
    "    Description\n",
    "    Calculates the forward propogation steps of the neural network\n",
    "    \n",
    "    Input\n",
    "    X_train   - the training examples\n",
    "        shape >> (number of features, n, x number of examples, m )\n",
    "    layers_dims - a list the dimensions of the neural networks layers\n",
    "        shape   >> (1 x number of layers, L)\n",
    "    \n",
    "    \n",
    "    Output\n",
    "    A_L    - the activation values of the final layer in the network\n",
    "    caches - caches of all of the Z and A values of the hidden layers which will be used in back prop\n",
    "    '''\n",
    "    \n",
    "    caches = {}\n",
    "    caches['A0'] = X_train\n",
    "    \n",
    "    for l in range(1, len(layers_dims)):\n",
    "        A_curr, [caches['Z' + str(l)], caches['A' + str(l)]] = forward_linear_activation(caches['A'+str(l-1)], \n",
    "                                                                    parameters['W' + str(l)], parameters['b' + str(l)], 'linear')\n",
    "        \n",
    "    return A_curr, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "for l in range(1,2):\n",
    "    print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.rand(10,5); layers_dims=[10,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "A, c = forward_prop(X_train, [784,9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n"
     ]
    }
   ],
   "source": [
    "print(c['A1'].shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(A_L):\n",
    "    '''\n",
    "    Description\n",
    "    Takes in the final values of the neural network and returns the predicted values \n",
    "    \n",
    "    Inputs\n",
    "    A_L    - the output of the final layer of the neural network\n",
    "    \n",
    "    \n",
    "    Outputs\n",
    "    Y_hat   - the predicted values - I know, not very descriptive... srry\n",
    "    \n",
    "    '''\n",
    "    Y_hat = np.zeros((1, A_L.shape[1]) ) \n",
    "    \n",
    "    for i in range(Y_hat.shape[1]):\n",
    "        Y_hat[0][i] = np.argmax(A_L[:, i])\n",
    "\n",
    "    return Y_hat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeCost(Y_hat, Y):\n",
    "    '''\n",
    "    Description\n",
    "    Computes the cost function for the linear classifier\n",
    "    \n",
    "    \n",
    "    Input\n",
    "    Y_hat - the predicted values for Y output by the neural network - shape = (1 x # examples)\n",
    "    Y     - the output examples that we use to train the algorithm  - shape = (1 x # examples)\n",
    "    \n",
    "    Ouptut\n",
    "    cost  - the cost of the current iteration of the algorithm. In this case, the L2 difference is used without \n",
    "        regularization\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    m = Y_hat.shape[1]\n",
    "    \n",
    "    cost = 1/m * np.dot( Y_hat - Y, (Y_hat - Y).T)\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_prop(parameters, X, Y_hat, Y, layers_dims, learning_rate = 0.01):\n",
    "    '''\n",
    "    Description\n",
    "    Calculates the gradients of the parameters\n",
    "    \n",
    "    Inputs\n",
    "    parameters - \n",
    "    Y_hat      - the predicted values\n",
    "    Y          - the known  values\n",
    "    \n",
    "    Outputs\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    l=1\n",
    "    m = Y_hat.shape[1]\n",
    "\n",
    "    \n",
    "    print('here I am')\n",
    "    \n",
    "    \n",
    "    dW = 2/m * np.dot(Y_hat - Y, X.T)\n",
    "    #print(\"dw \" + str(dW))\n",
    "    db = 2/m * (Y_hat - Y)\n",
    "    #print(dW)\n",
    " \n",
    "    \n",
    "#    for l in range(len(layers_dims), 1):\n",
    "    parameters['W' + str(l)] = parameters['W' + str(l)] - 100\n",
    "    parameters['b' + str(l)] = parameters['b' + str(l)] - learning_rate * db\n",
    "    \n",
    "    #print(parameters['W' + str(l)])\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "for l in range(2,0, -1):\n",
    "    print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, layers_dims, iterations = 20, print_cost = True):\n",
    "    '''\n",
    "    Description\n",
    "    Combines all aspects of the neural network into a single model\n",
    "\n",
    "    Inputs\n",
    "    \n",
    "    \n",
    "    Outputs\n",
    "    '''\n",
    "    \n",
    "    parameters = initialize(layers_dims)\n",
    "    #print('here I am')\n",
    "    \n",
    "    for i in range(iterations):\n",
    "\n",
    "        A_L, caches = forward_prop(X_train, parameters, layers_dims)\n",
    "\n",
    "        predictions = predict(A_L)\n",
    "\n",
    "        cost = computeCost(predictions, Y_train)\n",
    "\n",
    "        parameters = back_prop(parameters, X_train, predictions, Y_train, layers_dims, learning_rate = 0.01)\n",
    "        \n",
    "        if i % 10 == 0:\n",
    "            print('Cost for ' + str(i) + '= ' + str(cost) ) \n",
    "        if i ==5 : \n",
    "            print(predictions)\n",
    "    \n",
    "    final_prediction = predict(A_L)\n",
    "    \n",
    "    return final_prediction, parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here I am\n",
      "Cost for 0= [[14.4951]]\n",
      "here I am\n",
      "here I am\n",
      "here I am\n",
      "here I am\n",
      "here I am\n",
      "[[2. 2. 5. ... 2. 2. 2.]]\n",
      "here I am\n",
      "here I am\n",
      "here I am\n",
      "here I am\n",
      "here I am\n",
      "Cost for 10= [[14.4951]]\n",
      "here I am\n",
      "here I am\n",
      "here I am\n",
      "here I am\n",
      "here I am\n",
      "here I am\n",
      "here I am\n",
      "here I am\n",
      "here I am\n"
     ]
    }
   ],
   "source": [
    "m = model(X_train, y_train, [X_train.shape[0],9] , print_cost = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[2., 2., 5., ..., 2., 2., 2.]]), {'W1': array([[-0.04167578, -0.00562668, -0.21361961, ..., -0.06168445,\n",
      "         0.03213358, -0.09464469],\n",
      "       [-0.05301394, -0.1259207 ,  0.16775441, ..., -0.03284246,\n",
      "        -0.05623108,  0.01179136],\n",
      "       [ 0.07386378, -0.15872956,  0.01532001, ..., -0.08428557,\n",
      "         0.10040469,  0.00545832],\n",
      "       ...,\n",
      "       [ 0.24144681,  0.01110384, -0.01494575, ..., -0.0933672 ,\n",
      "         0.06313131,  0.05855765],\n",
      "       [ 0.05064269, -0.09990983, -0.03725472, ...,  0.08231032,\n",
      "        -0.05483214,  0.00767053],\n",
      "       [ 0.07695631, -0.02233986, -0.0742913 , ...,  0.01833128,\n",
      "         0.01864871, -0.07748973]]), 'b1': array([[0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.]])})\n"
     ]
    }
   ],
   "source": [
    "print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
